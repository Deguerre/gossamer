\documentclass{bioinfo}
\copyrightyear{2012}
\pubyear{2012}

\usepackage{multirow}
\def\Gossamer{\textit{Gossamer}}
\def\Translucent{\textit{Translucent}}
\def\Trinity{\textit{Trinity}}
\def\Oases{\textit{Oases}}
\def\Velvet{\textit{Velvet}}
\def\abinitio{\textit{ab initio}}
\def\denovo{\textit{de novo}}
\def\debruijn{\textit{de~Bruijn}}
\def\rhomer{$\rho$-mer}
\def\rhomers{$\rho$-mers}
\def\kmer{$k$-mer}
\def\kmers{$k$-mers}

\begin{document}
\firstpage{1}

\title[Translucent]{Translucent -- A Resource Efficient {\em de novo} Transcriptome Assembler}
\author[Bromage \textit{et~al}]{Andrew Bromage\footnote{to whom correspondence should be addressed}\ ,
        Thomas Conway,
        Jeremy Wazny,
        Bryan Beresford-Smith
        }
\address{NICTA Victoria Research Laboratory, Department of Computer Science and Software Engineering, The University of Melbourne, Parkville, Australia\\
}


\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}


\maketitle

\begin{abstract}

\section{Motivation:}

\section{Results:}

\section{Availability:}
\Translucent{} is available for non-commercial use 
from http://www.genomics.csse.unimelb.edu.au/product-gossamer.php.

\section{Contact:} \href{tom.conway@nicta.com.au}{tom.conway@nicta.com.au}
\end{abstract}

\section{Introduction}

The availability of low-cost high-throughput sequencing is making
RNA-seq an increasingly attractive tool for biological investigation
into transcriptionally-mediated processes.

In the past several years, there have been a number of systems reported
for assembly of transcripts.
They fall into two broad categories: \abinitio{}
methods, which use a reference genome to aid in the process of transcript
reconstruction, and reference-free \denovo{} methods, which attempt to
construct transcripts without a reference.

If a reference genome is available, \abinitio{} methods are generally
superior.
Reference-based assemblers can use information about adjacency in the
genome to assist in the assembly process, which can aid in the resolution
of some low-expression transcripts which a \denovo{} assembler would
inevitably miss, or in separating out sample contamination.
Splice junctions can complicate \abinitio{} assembly enormously.
While this problem is far from solved, significant headway has been made.

A reference genome is not always available, however, and
producing a finished genome is a time-consuming and
expensive process, even for organisms with a relatively small amount
of genomic DNA.

In some applications (e.g. analysing cancer cells not from a
well-researched cell line), the model under study may diverge from
the organism reference significantly such that the reference
genome is uninformative.
Moreover, it is often the deviations from the reference genome
which are precisely the features in which the investgator is
interested.

For these reasons, \denovo{} methods will continue to play a
significant role for some time to come.

The first \denovo{} approaches to RNA-seq assembly reused technology
developed for genomic assembly.
However, genomic assembly is based on a number of assumptions which
are not true of transcripts.

Genomic DNA is organised as a small number of long linear (or
circular) sequences which do not overlap.
This is not true of RNA, which is typically organised as a large
number of relatively short sequences, some of which have sequences
in common and some of which do not.

For genomic DNA, each sequence can be thought of as having coverage
that is close to uniform in a given set of reads.
Owing to chemical and physical biases in the sequencing pipeline, it
will not be exactly uniform, but the assumption is often good enough
that we can think of the sequencing experiment as having a fixed
``coverage''.
An experiment at 100 times coverage, for example, means that every
base in the genome is represented 100 times in the set of reads.

Again, this is not true of transcripts.
Different transcripts are expressed in different quantities, so
there is no notion of ``coverage''.
To complicate things further, there is essentially no way to tell
the difference between a read error and a low-expression alternative
isoform.

Not only are these assumptions not true in the context of RNA-seq,
they are sometimes precisely the features that an investigator is
interested in: a novel splicing event, or a difference
in expression of the same isoform.

In recent years, several tools have been developed for
performing \denovo{} assembly for RNA-seq data.
However, they tend to be very expensive compared with \abinitio{}
assemblers.

The main contribution of this paper is a new tool for \denovo{}
discovery of RNA-seq data, which is designed from the ground-up
for resource efficiency.

\section{Methods}

\subsection{De Bruijn graph construction}

\Translucent{} is based on the same technology as \Gossamer{} and shares
much of its infrastructure.

XXX How much detail do we really need to go into here?

\subsection{Error removal}

The simplest static filter is to remove any edge in the graph whose
coverage is less than some threshold.

There are two types of dynamic filter applied.
The first is that for each node in the graph, the out edges are
examined, and any edge whose coverage is less than some proportion
of the total coverage through the node is eliminated.
This is based on the observation that a transcript with low expression
is unlikely to be able to be resolved if it interferes with a transcript
with much higher expression.

The other kind of static filter is structural, and is based on
tip pruning and bubble popping (XXX citations).
Unlike the traditional implementations, we remove a path
only if the coverage of that path is less than some
proportion of the coverage of the other path.
We use the relative coverage because in high coverage regions,
errors are likely to be artificially amplified.

\subsection{Read clustering}

After likely errors have been removed from the \debruijn{} graph,
the reads are clustered together based on how likely they are to
have arisen from the same locus.
The process is broadly similar to that of \Trinity{}:
We first greedily extract high-coverage contigs, then look for opportunities
to cluster contigs together using information about reads which connect
the two contigs together.

To save memory, we compress the contigs as they are generated using
the a method similar to that suggested in (XXX original paper).

Each contig may be understood as a walk in the \debruijn{} graph.
At each node, there are at most four possible outgoing edges, but
the actual number is typically smaller.
We can therefore represent the contig by storing the initial
\kmer{}, the length of the contig, and the sequence of choices
which must be made along the walk.

If there is only one outgoing edge from a node, then there is
no choice to be made, and so the following edge can be represented
with zero bits.
If there are two outgoing edges, then there is only a binary choice,
and the following edge can be represented with one bit.
Otherwise, two bits must be used.
Using this method, most contigs compress to less than one byte
(plus the initial \kmer{} and length), and so may be stored in
primary memory.

Once the clusters have been identified, each read is then assigned
to a cluster, based on how many graph edges it has in common with
that cluster.

\subsection{Transcript assembly}

For each cluster of reads, \Translucent{} builds a local \debruijn{} graph
out of those reads.
We call this graph a ``component''.

\subsubsection{Breaking cycles}

Strongly-connected components are found, then cycle breaking is
performed on each of these.
There are three cycle-breaking tactics, chosen based on
on how complex the structure of the SCC is.

If the SCC contains a single cycle, then a simple tactic is applied.
Here, we locate the edge which would, if removed, both break the
cycle and maximise the length of resulting linear paths, and
remove that edge.

If the SCC has more than one cycle, then the graph is examined to
see how complex it is, using the number of distinct linear paths
and the number of junction nodes.
If it is not too complex, then
we locate the edge which would, if removed, break the largest number
of cycles, and remove that edge.

If it is too complex, then the previous method would be too expensive
to compute.
In this situation, low-coverage edges are aggressively pruned
until at least one cycle has been broken.
This ``last resort'' tactic can result in low-coverage transcripts
being fragmented.
However it only occurs in components with a very high complexity; it
rarely happens more than a few times in a given experiment, and even
then, only when the \kmer{} size is low.
Using multiple \kmer{} sizes (see Section~\ref{sect:multik}) ameliorates
any problems caused by applying this tactic in practice.

These tactics are applied repeatedly until all cycles are broken.

\subsubsection{Transcript extraction}

XXX Should the discussion of consensus set methods be moved to the
introduction? I think we need to keep the justification for not
using such a method. Moving it to the introduction would allow us
to mention methods based on Dilworth's theorem, too.

Previous \denovo{} transcript assemblers typically use a dynamic
programming approach for extracting a consensus set of transcripts
from a branched partial order graph, similar to~\cite{Lee:2003}.
The theory behind such methods is that if a consensus
exists, the a set of sequences can be obtained by finding
the maximum likelihood path (in an appropriate model), removing
that path, and iterating until the entire graph is covered.

These approaches were developed for \abinitio{} assembly, where the
assumption that a consensus sequence \textit{exists} is justified by
the fact that the graph was generated using alignment.
Homology between the read evidence and the underlying organism is
established \textit{before} likely transcripts are identified.

In the context of \denovo{} assembly, this assumption cannot be
assumed to hold.

Multiple sequence alignment acts as a filter which helps remove
reads or correct which are not homologous to the reference organism.
A \denovo{} assembler does not have this filtering step, so
read contamination, or errors which have been amplified
by the biochemistry, have been eliminated.

Once the highest-likelihood transcripts have been found and extracted
from the graph, a consensus method will then attempt to piece
lower-likelihood transcripts together.
The evidence for these lower-likelihood transcripts is fundamentally
weaker, because it is computed from the original graph with the
higher-likelihood transcripts removed.
In the presence of sample contamination or read errors which have
not been removed, real transcripts may be fragmented (if
the method is too aggressive) or real transcript fragments may
be pasted to erroneous fragments (if the method is too lenient).

Given these problems, we adopt a different approach: we extract all
paths in the graph which have read evidence to support them.

The algorithm is very simple. First, we seed every junction node in the
de~Bruijn graph with an empty transcript, then visit the nodes in
topological order, at each stage extending transcripts along input
edges if there is sufficient read evidence to support that extension.

After tracing through the graph in a single pass, we perform
transitive reduction on that set, removing a transcript if is
a subsequence of some other transcript.

\subsubsection{Expression estimation}

To estimate expression levels, \Translucent{} aligns all of the
for which were assigned to the component to the extracted transcripts.
We use the same approach as in Cufflinks: if a read could map to
more than one location, share it out in proportion to the number
of places it could map.

XXX We could use~\cite{Li:2010}. Or we could use the espresso method.

XXX There is a good question to be asked as to whether or not expression
estimation even makes sense.
Since \denovo{} methods inevitably produce transcripts which are truncated
or fragmented, the factor of length of the transcript may dominate
the calculation, producing estimates which could be wildly inaccurate.

\subsection{Multiple-$k$}\label{sect:multik}

As pointed out by~\cite{Simpson:2009}, the appropriate choice of
\kmer{} size for a given sequencing experiment depends on the depth
of sequence coverage, the read error rate, and the complexity of
the underlying sequence.

In \denovo{} mRNA discovery, transcripts are generally expressed
at different levels; since there is no one ``sequence coverage'',
there is no one optimal value of~$k$.
As~\cite{SurgetGroba:2010}
notes, the standard deviation of transcript abundances increases
as~$k$ increases, so using a small~$k$ tends to produce more
transcripts with a low range of abundances, and using a large~$k$
tends to produce fewer transcripts with a high range of abundances.

We therefore support assembly with multiple \kmer{} sizes.
The approach is broadly similar to~\Oases{} in that the~\Translucent{}
pipeline is run multiple times with different values of~$k$, the
transcripts are merged into a new de~Bruijn graph, and the~\Translucent{}
pipeline is run again on that graph.

\section{Results}

\begin{table}
\label{tab:results}
\begin{tabular}{|r|r|r|}
\hline
Metric & Trinity & Translucent \\
\hline
Number of transcripts $\ge$ 100bp &  54,941 & 73,250 \\
Number of transcripts $\ge$ 350bp &    6125 &   4799 \\
Transcript matches \@ 80\%        &  36,226 & 59,323 \\
Transcript matches \@ 90\%        &  32,902 & 58,096 \\
Transcript matches \@ 100\%       &  15,476 & 31,362 \\
Reference matches \@ 80\%         &  27,558 & 16,734 \\
Reference matches \@ 90\%         &  21,170 & 11,180 \\
Reference matches \@ 100\%        &    8015 &   3121 \\
\hline
\end{tabular}
\caption{A comparison of quality results from \Trinity{} and \Translucent{}.}
\end{table}

\begin{table}
\label{tab:efficiency}
\begin{tabular}{|r|r|r|}
\hline
 & Trinity & Translucent \\
\hline
Time & 7 days & 3 days \\
Peak RAM usage & 29GB & 7GB \\
\hline
\end{tabular}
\caption{A comparison of time and space usage.}
\end{table}

\textit{Mus musculus}, accession number SRR203276. The reference
transcripts are NCBIM37.66.
The set contains approximately 53~million read pairs,
of length~76.

We did attempt to compare against \Oases{}, but unfortunately, the
pipeline exhausted the amount of RAM on our test machine (32~GB).
(XXX citation) reports that \Oases{} uses far less memory on this
size of data set.
This may be the case, however~\Oases{} requires that~\Velvet{} be run
on the data first, and this was not possible on our machine.

We therefore only compared \Translucent{} against \Trinity{}.

The quality results are shown in Table~\ref{tab:results}.
``Transcript matches \@ N\%'' is the number of recovered transcripts
which align to known transcripts in the reference set, where the
number of identical bases is at least N\% of the transcript size.
``Reference matches \@ N\%''
is the number of reference transcripts which align to recovered transcripts,
where the number of identical bases is at least N\% of the reference size.

The time and space usage is reported in Figure~\ref{tab:efficiency}.
We note that \Trinity{} only assembles at one \kmer{} size, where
the \Translucent{} results include five separate assemblies at
different $k$ ($k=19,23,27,31,35$), plus a final assembly on the
merged results.

\section{Discussion}

\section*{Acknowledgement}

National ICT Australia (NICTA) is funded by the Australian Government's Department of Communications; 
Information Technology and the Arts;  
Australian Research Council through Backing Australia's Ability; 
ICT Centre of Excellence programs.

\bibliographystyle{natbib}
\bibliography{paper}

\end{document}
